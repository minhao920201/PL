{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb5cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter you want to crawl how many pages:1\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req #引入urllib模組中函式名稱為request的函式為req\n",
    "import bs4 #引入bs4模組\n",
    "import re\n",
    "#定義函式getData，用以取得每一頁所有標題、標籤及內文。參數為網址\n",
    "def getData(url,count,pages):\n",
    "    #建立一個Request物件，附加Request Headers的資訊\n",
    "    request = req.Request(url,headers = {\n",
    "        \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "    \n",
    "    #打開request\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "\n",
    "    root = bs4.BeautifulSoup(data,\"html.parser\") #讓BeautifulSoup解析HTML格式文件\n",
    "    all_titles = root.find_all(\"h3\",class_=\"qa-list__title\") #以列表形式找出所有class_=title的h3標籤\n",
    "\n",
    "\n",
    "    #開啟finalproject.txt並用a模式保留原本資料並將新的資料寫入\n",
    "    with open(\"jsonpy.json\",mode = \"a\",encoding = \"utf-8\") as ftext:\n",
    "        #此迴圈會找出該頁所有標題、標籤及內文\n",
    "        for title in all_titles:\n",
    "            title.a.string = re.sub(r\"\\\"\",\"'\",title.a.string)\n",
    "            title.a.string = re.sub(r\"\\\\\",\" BACKSLASH \",title.a.string)\n",
    "            ftext.write(\"\\n   {\\n\")\n",
    "            ftext.write(\"   \\\"標題\\\":\"+\"\\\"\"+title.a.string+\"\\\",\"+\"\\n\") #將內文的標題寫入finalproject.txt裡\n",
    "            ftext.write(\"   \\\"標籤\\\":[\")\n",
    "\n",
    "            url2 = title.a[\"href\"] #將內文的網址存到url2裡\n",
    "\n",
    "            #建立一個Request物件，附加Request Headers的資訊\n",
    "            request2 = req.Request(url2,headers = {\n",
    "                \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36\"\n",
    "            })\n",
    "\n",
    "            #打開request2\n",
    "            with req.urlopen(request2) as response2:\n",
    "                data2 = response2.read().decode(\"utf-8\")\n",
    "            \n",
    "            root2 = bs4.BeautifulSoup(data2,\"html.parser\") #讓BeautifulSoup解析HTML格式文件\n",
    "            contents = root2.find(\"div\",class_=\"markdown__style\").text #找出內文\n",
    "            tags = root2.find(\"div\",class_=\"qa-header__tagGroup\") #以列表形式找出所有 class_=qa-header__tagGroup 的 div 標籤\n",
    "            time_and_view = root2.find(\"div\",class_=\"qa-header__info\").text.split()\n",
    "            #將標籤寫入檔案中並讓每個標籤名稱以空格作為間隔\n",
    "            '''\n",
    "            for i in tags:\n",
    "                if (i.text!=\"\\n\"):\n",
    "                    ftext.write(\"\\\"\" + i.text + \"\\\",\")\n",
    "            '''\n",
    "            contents = re.sub(r\"\\\"\",\"'\",contents)\n",
    "            contents = re.sub(r\"\\\\\",\" BACKSLASH \",contents)\n",
    "            contents = re.sub(r\"\\n\",\"EOL\",contents)\n",
    "            contents = re.sub(r\"\\t\",\"    \",contents)\n",
    "\n",
    "            set_of_tags = set(tags.text.split(\"\\n\"))\n",
    "            list_of_tags = []\n",
    "            for i in set_of_tags:\n",
    "                list_of_tags.append(i)\n",
    "            for tag in list_of_tags[1:]:\n",
    "                tag = re.sub(r\"\\\\\",\" BACKSLASH \",tag)\n",
    "                if (tag!=list_of_tags[-1]):\n",
    "                    ftext.write(\"\\\"\" + tag + \"\\\",\")\n",
    "                else:\n",
    "                    ftext.write(\"\\\"\" + tag + \"\\\"],\\n\")\n",
    "            ftext.write(\"   \\\"詢問時間\\\":\\\"\"+time_and_view[1]+\"\\\",\\n\")\n",
    "            ftext.write(\"   \\\"瀏覽次數\\\":\\\"\"+time_and_view[-2]+\"\\\",\\n\")\n",
    "\n",
    "            if ( (count==pages-1) and (title.a.string == all_titles[-1].a.string)):\n",
    "                ftext.write(\"   \\\"內文\\\":\\\"\"+contents+\"\\\"\\n\") #將內文寫入content.txt裡\n",
    "                ftext.write(\"   }]\\n}\")\n",
    "            else:\n",
    "                ftext.write(\"   \\\"內文\\\":\\\"\"+contents+\"\\\"\\n\")\n",
    "                ftext.write(\"   },\\n\")\n",
    "\n",
    "\n",
    "    #抓取下一頁的連結\n",
    "    nextLink = root.find(\"a\",string=\"下一頁\") #找到內文是 下一頁 的標籤a\n",
    "    return nextLink[\"href\"] #回傳下一頁的網址\n",
    "\n",
    "with open(\"jsonpy.json\",mode = \"a\",encoding = \"utf-8\") as ftext:\n",
    "    ftext.write(\"{\\n\")\n",
    "    ftext.write(\"   \\\"Article\\\":[\")\n",
    "pageURL = \"https://ithelp.ithome.com.tw/\" #將起始網頁的網址存到pageURL裡\n",
    "count = 0 #count用來表示想抓取幾頁\n",
    "pages = int(input(\"Please enter you want to crawl how many pages:\"))\n",
    "#重複抓取每頁資料\n",
    "while count<pages:\n",
    "    pageURL = getData(pageURL,count,pages) #將 return 回來的網址覆蓋到pageURL上，以便重複利用\n",
    "    count += 1 #執行完一次後增加1，以確保能抓到預期的頁數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a681de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
